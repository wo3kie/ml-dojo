{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a333d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Project:\n",
    "#      PyTorch Dojo (https://github.com/wo3kie/ml-dojo)\n",
    "#\n",
    "# Author:\n",
    "#      Lukasz Czerwinski (https://www.lukaszczerwinski.pl/)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c3a588",
   "metadata": {},
   "source": [
    "$$ H(X) = \\sum_{e \\in \\mathcal{E}} P(X=e) \\, log_2 \\left( \\frac{1}{P(X=e)} \\right) $$\n",
    "$$ \\\\[2em]$$\n",
    "$$ H(X) = \\text{How many bits on average} $$\n",
    "$$ \\\\[2em]$$\n",
    "$$ \\text{Example} $$\n",
    "$$ X = \\Bigg[ \\frac{1}{2}, \\frac{1}{4}, \\frac{1}{8}, \\cdots \\Bigg] $$\n",
    "$$ H(X) = \\frac{1}{2} \\log_2(2) + \\frac{1}{4} \\log_2(4) + \\frac{1}{8} \\log_2(8) + \\cdots = $$\n",
    "$$ \\frac{1}{2} \\cdot 1 + \\frac{1}{4} \\cdot 2 + \\frac{1}{8} \\cdot 3 + \\cdots = $$\n",
    "$$ 2 $$\n",
    "$$ \\text{1 bit frequently (50\\%), 2 bits rarely (25\\%), 3 bits even more rarely (12.5\\%), ... } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93beff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import long, manual_seed, randn, Tensor\n",
    "\n",
    "import import_ipynb\n",
    "import common # type: ignore\n",
    "from common import assert_eq, assert_ne, T # type: ignore\n",
    "\n",
    "import cross_entropy # type: ignore\n",
    "\n",
    "\n",
    "def entropy(iterable, count=False, norm=False):\n",
    "    \"\"\"\n",
    "    Calculates the entropy of samples when `count` is True, or the entropy of samples otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    iterable = T(iterable)\n",
    "\n",
    "    if count == True:\n",
    "        iterable = common.count(iterable)[1]\n",
    "\n",
    "    if (count == True) or (norm == True):\n",
    "        iterable = iterable / iterable.sum()\n",
    "\n",
    "    return cross_entropy._cross_entropy_d(iterable, iterable)\n",
    "    \n",
    "\n",
    "def test_entropy_1():\n",
    "    #\n",
    "    # The entropy of the certain distribution (0% / 100%) is 0.\n",
    "    #\n",
    "\n",
    "    assert_eq(entropy([1]), 0.0, atol=0.01)\n",
    "    # assert_eq(entropy([0]), 0.0, atol=0.01) --- IGNORE ---\n",
    "\n",
    "    assert_eq(entropy([0, 1]), 0.0, atol=0.01)\n",
    "    assert_eq(entropy([1, 0]), 0.0, atol=0.01)\n",
    "\n",
    "\n",
    "def test_entropy_2():\n",
    "    #\n",
    "    # The entropy of a distribution is maximum when all elements are equal,\n",
    "    # and in that case it is equal to log2(n) where n is the number of elements in the distribution.\n",
    "    #\n",
    "\n",
    "    assert_eq(entropy([1/2, 1/2]), 1.0, atol=0.01)\n",
    "    assert_eq(entropy([1/4, 1/4, 1/4, 1/4]), 2.0, atol=0.01)\n",
    "    assert_eq(entropy([1/8, 1/8, 1/8, 1/8, 1/8, 1/8, 1/8, 1/8]), 3.0, atol=0.01)\n",
    "\n",
    "    #\n",
    "    # By smoothing the distribution we can increase the entropy, \n",
    "    # but it cannot exceed log2(n) where n is the number of elements in the distribution.\n",
    "    #\n",
    "\n",
    "    assert_eq(entropy([1/2, 1/2]), 1.000, atol=0.01)\n",
    "    assert_eq(entropy([1/2, 1/4, 1/4]), 1.500, atol=0.01)\n",
    "    assert_eq(entropy([1/2, 1/4, 1/8, 1/8]), 1.750, atol=0.01)\n",
    "    assert_eq(entropy([1/4, 1/4, 1/4, 1/8, 1/8]), 2.250, atol=0.01)\n",
    "    assert_eq(entropy([1/4, 1/4, 1/8, 1/8, 1/8, 1/8]), 2.500, atol=0.01)\n",
    "    assert_eq(entropy([1/4, 1/8, 1/8, 1/8, 1/8, 1/8, 1/8]), 2.750, atol=0.01)\n",
    "    assert_eq(entropy([1/8, 1/8, 1/8, 1/8, 1/8, 1/8, 1/8, 1/8]), 3.0, atol=0.01)\n",
    "\n",
    "\n",
    "def test_entropy_3():\n",
    "    #\n",
    "    # Geometric sequence, entropy(1/2, 1/4, 1/8, ...) -> 2.0\n",
    "    #\n",
    "\n",
    "    assert_eq(entropy([1/2, 1/4, 1/8, 1/16, 1/32, 1/64, 1/128, 1/256, 1/512, 1/1024], norm=True), 1.988, atol=0.01)\n",
    "\n",
    "\n",
    "def test_entropy_4():\n",
    "    #\n",
    "    # Heavy-tail distributions have lower entropy than uniform distribution.\n",
    "    #\n",
    "\n",
    "    assert_eq(entropy([3/12, 3/12, 3/12, 3/12]), 2.0, atol=0.01)\n",
    "    assert_eq(entropy([4/12, 2/12, 3/12, 3/12]), 1.959, atol=0.01)\n",
    "    assert_eq(entropy([5/12, 1/12, 3/12, 3/12]), 1.825, atol=0.01)\n",
    "    assert_eq(entropy([6/12, 1/12, 2/12, 3/12]), 1.723, atol=0.01)\n",
    "    assert_eq(entropy([7/12, 1/12, 1/12, 3/12]), 1.551, atol=0.01)\n",
    "    assert_eq(entropy([8/12, 1/12, 1/12, 2/12]), 1.418, atol=0.01)\n",
    "    assert_eq(entropy([9/12, 1/12, 1/12, 1/12]), 1.207, atol=0.01)\n",
    "\n",
    "\n",
    "def test_entropy_5():\n",
    "    #\n",
    "    # Zipf's distribution has dominant element (decrease entropy),\n",
    "    # and the tail of the distribution is long (increase entropy),\n",
    "    # therefore the entropy of Zipf's distribution is high but not maximum.\n",
    "    #\n",
    "\n",
    "    assert_eq(entropy([1/4, 1/4, 1/4, 1/4], norm=True), 2.0, atol=0.01)\n",
    "    assert_eq(entropy([1/1, 1/2, 1/3, 1/4], norm=True), 1.7924, atol=0.01)\n",
    "\n",
    "    assert_eq(entropy([1/8, 1/8, 1/8, 1/8, 1/8, 1/8, 1/8, 1/8], norm=True), 3.0, atol=0.01)\n",
    "    assert_eq(entropy([1/1, 1/2, 1/3, 1/4, 1/5, 1/6, 1/7, 1/8], norm=True), 2.6197, atol=0.01)\n",
    "\n",
    "\n",
    "def test_entropy_6():\n",
    "    #\n",
    "    # Uniform distribution, the higher standard deviation, the (logarithmically) higher entropy.\n",
    "    #\n",
    "    \n",
    "    normal = lambda size, std: (std * randn(size)).round().to(long)\n",
    "\n",
    "    manual_seed(0)\n",
    "    assert_eq(entropy(normal(size=1024,   std=1.0), count=True), 2.117, atol=0.01)\n",
    "    assert_eq(entropy(normal(size=1024,   std=5.0), count=True), 4.356, atol=0.01)\n",
    "    assert_eq(entropy(normal(size=1024,  std=10.0), count=True), 5.258, atol=0.01)\n",
    "    assert_eq(entropy(normal(size=1024,  std=25.0), count=True), 6.512, atol=0.01)\n",
    "    assert_eq(entropy(normal(size=1024,  std=50.0), count=True), 7.473, atol=0.01)\n",
    "    assert_eq(entropy(normal(size=1024, std=100.0), count=True), 8.278, atol=0.01)\n",
    "\n",
    "\n",
    "def test_entropy_7():\n",
    "    entropy_from_string = lambda s: entropy(list(map(ord, s)), count=True)\n",
    "\n",
    "    assert_eq(entropy_from_string(\"aaaaaaaaaaaaaaaa\"), 0.000, atol=0.01)\n",
    "    assert_eq(entropy_from_string(\"aaaaaaaaaaaaaabb\"), 0.543, atol=0.01)\n",
    "    assert_eq(entropy_from_string(\"aaaaaaaaaaaabbbb\"), 0.811, atol=0.01)\n",
    "    assert_eq(entropy_from_string(\"aaaaaaaaaabbbbbb\"), 0.954, atol=0.01)\n",
    "    assert_eq(entropy_from_string(\"aaaaaaaabbbbbbbb\"), 1.000, atol=0.01)\n",
    "\n",
    "    \n",
    "#\n",
    "# it execute this code when running %run command, but not when using import_ipynb\n",
    "#\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_entropy_1()\n",
    "    test_entropy_2()\n",
    "    test_entropy_3()\n",
    "    test_entropy_4()\n",
    "    test_entropy_5()\n",
    "    test_entropy_6()\n",
    "    test_entropy_7()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
