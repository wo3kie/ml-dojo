{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde64ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Project:\n",
    "#      PyTorch Dojo (https://github.com/wo3kie/ml-dojo)\n",
    "#\n",
    "# Author:\n",
    "#      Lukasz Czerwinski (https://www.lukaszczerwinski.pl/)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25444911",
   "metadata": {},
   "source": [
    "$$ loss(w, b) = L(w, b) = \\frac{1}{S} \\sum_{i=1}{(y_i - (x_i w + b)) ^ 2} = $$\n",
    "$$ \\frac{1}{S} \\sum_{i=1}{2 \\, (y_i - (x_i w + b)) \\, x_i} = $$\n",
    "$$ \\frac{2}{S} \\sum_{i=1}{(y_i - (x_i w + b)) \\, x_i} = $$\n",
    "$$ \\frac{2}{S} \\sum_{i=1}{(y_i - predicted_i) \\,x_i} = $$\n",
    "$$ \\frac{2}{S} \\sum_{i=1}{error_i \\, x_i} = $$\n",
    "$$ \\frac{2}{S} \\, x^T \\cdot error $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9e29a3",
   "metadata": {},
   "source": [
    "$$ loss(w, b) = L(w, b) = \\frac{1}{S} \\sum_{i=1}^{S} (y_i - (x_i w + b)) ^ 2 = $$\n",
    "$$ \\frac{1}{S} \\sum_{i=1}^{S} 2 \\, (y_i - (x_i w + b)) \\, 1= $$\n",
    "$$ \\frac{2}{S} \\sum_{i=1}^{S} ((x_i w + b) - y_i) = $$\n",
    "$$ \\frac{2}{S} \\sum_{i=1}^{S} (predicted_i - y_i) = $$\n",
    "$$ \\frac{2}{S} \\sum_{i=1}^{S} error_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e5ac2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'assert_eq' from 'common' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m full, manual_seed, randn, Size, Tensor\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimport_ipynb\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m assert_almost, assert_eq, T \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlinear_regression_sgd_gradient\u001b[39m(X: Tensor, y: Tensor, epochs=\u001b[32m1000\u001b[39m, lr=\u001b[32m0.1\u001b[39m) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mfloat\u001b[39m, Tensor, Tensor]:\n\u001b[32m      8\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    Performs linear regression using Stochastic Gradient Descent (SGD) with manual gradient calculation.\u001b[39;00m\n\u001b[32m     10\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m \u001b[33;03m        tuple: (loss, weights, bias)\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'assert_eq' from 'common' (unknown location)"
     ]
    }
   ],
   "source": [
    "from torch import full, manual_seed, randn, Size, Tensor\n",
    "\n",
    "import import_ipynb\n",
    "from common import assert_eq, T # type: ignore\n",
    "\n",
    "\n",
    "def linear_regression_sgd_gradient(X: Tensor, y: Tensor, epochs=1000, lr=0.1) -> tuple[float, Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Performs linear regression using Stochastic Gradient Descent (SGD) with manual gradient calculation.\n",
    "\n",
    "    Parameters:\n",
    "        X: Input features of shape (S, F)\n",
    "        y: Target values of shape (S, 1)\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "\n",
    "    Returns:\n",
    "        tuple: (loss, weights, bias)\n",
    "    \"\"\"\n",
    "\n",
    "    (S, F) = X.shape\n",
    "\n",
    "    assert_eq(X.shape, Size([S, F]))\n",
    "    assert_eq(y.shape, Size([S, 1]))\n",
    "\n",
    "    w = randn((F, 1))\n",
    "    b = randn(1)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        predicted = X @ w + b\n",
    "        assert_eq(predicted.shape, Size([S, 1]))\n",
    "\n",
    "        error = predicted - y\n",
    "        assert_eq(error.shape, Size([S, 1]))\n",
    "\n",
    "        dL_dW = (2/S) * X.T @ error\n",
    "        assert_eq(dL_dW.shape, Size([F, 1]))\n",
    "\n",
    "        w = w - lr * dL_dW\n",
    "        assert_eq(w.shape, Size([F, 1]))\n",
    "\n",
    "        dL_db = (2/S) * error.sum()\n",
    "        assert_eq(dL_db.shape, Size([]))\n",
    "\n",
    "        b = b - lr * dL_db\n",
    "        assert_eq(b.shape, Size([1]))\n",
    "\n",
    "        #\n",
    "        # In the autograd version, computing the loss is required because it serves \n",
    "        # as the root of the computational graph for backpropagation. \n",
    "        #\n",
    "        # In the manualâ€‘gradient version, the loss value is not needed for the weight update itself,\n",
    "        # it is computed only to monitor training progress.\n",
    "        #\n",
    "        \n",
    "        loss = 1/S * (error ** 2).sum()\n",
    "        assert_eq(loss.shape, Size([]))\n",
    "\n",
    "    return (loss.item(), w, b)\n",
    "\n",
    "\n",
    "def _test_linear_regression_sgd_gradient(S: int, W: Tensor, B: Tensor, epochs=1000, lr=0.1, atol=0.01) -> None:\n",
    "    \"\"\"\n",
    "    Tests the linear regression using Stochastic Gradient Descent (SGD) with manual gradient calculation, \n",
    "    by generating synthetic data with known weights, and verifies that the computed weights and bias are correct.\n",
    "\n",
    "    Parameters:\n",
    "        S: Number of samples\n",
    "        W: Model's weight(s)\n",
    "        B: Model's bias\n",
    "    \"\"\"\n",
    "\n",
    "    F = W.shape[0]\n",
    "    x = randn(S, F)\n",
    "    assert_eq(x.shape, Size([S, F]))\n",
    "\n",
    "    y = x @ W + B\n",
    "    assert_eq(y.shape, Size([S, 1]))\n",
    "\n",
    "    loss, w, b = linear_regression_sgd_gradient(x, y, epochs, lr)\n",
    "    assert_eq(loss, 0.0, atol)\n",
    "    assert_eq(b, B, atol)\n",
    "    assert_eq(w, W, atol)\n",
    "\n",
    "\n",
    "def test_linear_regression_sgd_gradient() -> None:\n",
    "    _test_linear_regression_sgd_gradient(10, W=T([[0.1]]), B=0.2)\n",
    "    _test_linear_regression_sgd_gradient(10, W=T([[0.3], [0.4]]), B=0.5)\n",
    "    _test_linear_regression_sgd_gradient(10, W=T([[0.6], [0.7], [0.8]]), B=0.9)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_linear_regression_sgd_gradient()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
