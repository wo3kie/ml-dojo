{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde64ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Project:\n",
    "#      PyTorch Dojo (https://github.com/wo3kie/ml-dojo)\n",
    "#\n",
    "# Author:\n",
    "#      Lukasz Czerwinski (https://www.lukaszczerwinski.pl/)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25444911",
   "metadata": {},
   "source": [
    "$$ loss(w, b) = L(w, b) = \\frac{1}{S} \\sum_{i=1}{(y_i - (x_i w + b)) ^ 2} = $$\n",
    "$$ \\frac{1}{S} \\sum_{i=1}{2 \\, (y_i - (x_i w + b)) \\, x_i} = $$\n",
    "$$ \\frac{2}{S} \\sum_{i=1}{(y_i - (x_i w + b)) \\, x_i} = $$\n",
    "$$ \\frac{2}{S} \\sum_{i=1}{(y_i - predicted_i) \\,x_i} = $$\n",
    "$$ \\frac{2}{S} \\sum_{i=1}{error_i \\, x_i} = $$\n",
    "$$ \\frac{2}{S} \\, x^T \\cdot error $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9e29a3",
   "metadata": {},
   "source": [
    "$$ loss(w, b) = L(w, b) = \\frac{1}{S} \\sum_{i=1}^{S} (y_i - (x_i w + b)) ^ 2 = $$\n",
    "$$ \\frac{1}{S} \\sum_{i=1}^{S} 2 \\, (y_i - (x_i w + b)) \\, 1= $$\n",
    "$$ \\frac{2}{S} \\sum_{i=1}^{S} ((x_i w + b) - y_i) = $$\n",
    "$$ \\frac{2}{S} \\sum_{i=1}^{S} (predicted_i - y_i) = $$\n",
    "$$ \\frac{2}{S} \\sum_{i=1}^{S} error_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e5ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import randn, Size\n",
    "\n",
    "# %run common.ipynb\n",
    "import import_ipynb\n",
    "from common import equal, T # type: ignore\n",
    "\n",
    "\n",
    "def linear_regression_SGD_gradient(X, y, epochs=2000, lr=0.01):\n",
    "    (S, F) = X.shape\n",
    "\n",
    "    assert X.shape == Size([S, F])\n",
    "    assert y.shape == Size([S, 1])\n",
    "\n",
    "    w = randn((F, 1))\n",
    "    b = randn(1)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        predicted = X @ w + b\n",
    "        assert(predicted.shape == Size([S, 1]))\n",
    "\n",
    "        error = predicted - y\n",
    "        assert(error.shape == Size([S, 1]))\n",
    "\n",
    "        loss = 1/S * (error ** 2).sum()\n",
    "        assert(loss.shape == Size([]))\n",
    "\n",
    "        dL_dW = (2/S) * X.T @ error\n",
    "        assert(dL_dW.shape == Size([F, 1]))\n",
    "\n",
    "        w = w - lr * dL_dW\n",
    "        assert(w.shape == Size([F, 1]))\n",
    "\n",
    "        dL_db = (2/S) * (error).sum()\n",
    "        assert(dL_db.shape == Size([]))\n",
    "\n",
    "        b = b - lr * dL_db\n",
    "        assert(b.shape == Size([1]))\n",
    "\n",
    "    return (loss, w, b)\n",
    "\n",
    "\n",
    "def _test_linear_regression_SGD_gradient(S, W, B, epochs=2000, lr=0.01):\n",
    "    \"\"\"\n",
    "    Tests the linear regression using Stochastic Gradient Descent (SGD) with manual gradient calculation, \n",
    "    by generating synthetic data with known weights, and verifies that the computed weights and bias are correct.\n",
    "\n",
    "    Parameters:\n",
    "        S (float): Samples\n",
    "        W (float): Model's weight(s)\n",
    "        B (float): Model's bias\n",
    "    \"\"\"\n",
    "\n",
    "    F = W.shape[0]\n",
    "    x = randn(S, F)\n",
    "    assert(x.shape == Size([S, F]))\n",
    "\n",
    "    y = x @ W + B\n",
    "    assert(y.shape == Size([S, 1]))\n",
    "\n",
    "    loss, w, b = linear_regression_SGD_gradient(x, y, epochs, lr)\n",
    "    assert(equal(loss, 0.0))\n",
    "    assert(equal(b, B))\n",
    "    assert(equal(w, W))\n",
    "\n",
    "\n",
    "def test_linear_regression_SGD_gradient():\n",
    "    # 1 out of 10 times the test fails due to random initialization of weights and bias, \n",
    "    # which can lead to non-convergence in some cases. Simply re-run the test to pass it.\n",
    "\n",
    "    _test_linear_regression_SGD_gradient(10, W=T([[0.1]]), B=0.2)\n",
    "    _test_linear_regression_SGD_gradient(10, W=T([[0.3], [0.4]]), B=0.5)\n",
    "    _test_linear_regression_SGD_gradient(10, W=T([[0.6], [0.7], [0.8]]), B=0.9)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_linear_regression_SGD_gradient()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
