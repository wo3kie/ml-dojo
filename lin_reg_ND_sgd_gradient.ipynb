{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde64ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Project:\n",
    "#      PyTorch Dojo (https://github.com/wo3kie/ml-dojo)\n",
    "#\n",
    "# Author:\n",
    "#      Lukasz Czerwinski (https://www.lukaszczerwinski.pl/)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25444911",
   "metadata": {},
   "source": [
    "$$ loss(w, b) = L(w, b) = \\frac{1}{S} \\sum_{i=1}{(y_i - (x_i w + b)) ^ 2} = $$\n",
    "$$ \\frac{1}{S} \\sum_{i=1}{2 \\, (y_i - (x_i w + b)) \\, x_i} = $$\n",
    "$$ \\frac{2}{S} \\sum_{i=1}{(y_i - (x_i w + b)) \\, x_i} = $$\n",
    "$$ \\frac{2}{S} \\sum_{i=1}{(y_i - predicted_i) \\,x_i} = $$\n",
    "$$ \\frac{2}{S} \\sum_{i=1}{error_i \\, x_i} = $$\n",
    "$$ \\frac{2}{S} \\, x^T \\cdot error $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9e29a3",
   "metadata": {},
   "source": [
    "$$ loss(w, b) = L(w, b) = \\frac{1}{S} \\sum_{i=1}^{S} (y_i - (x_i w + b)) ^ 2 = $$\n",
    "$$ \\frac{1}{S} \\sum_{i=1}^{S} 2 \\, (y_i - (x_i w + b)) \\, 1= $$\n",
    "$$ \\frac{2}{S} \\sum_{i=1}^{S} ((x_i w + b) - y_i) = $$\n",
    "$$ \\frac{2}{S} \\sum_{i=1}^{S} (predicted_i - y_i) = $$\n",
    "$$ \\frac{2}{S} \\sum_{i=1}^{S} error_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e5ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import full, manual_seed, randn, Size, Tensor\n",
    "\n",
    "import import_ipynb\n",
    "from common import equal, T # type: ignore\n",
    "\n",
    "\n",
    "def linear_regression_sgd_gradient(\n",
    "        X: Tensor, \n",
    "        y: Tensor, \n",
    "        epochs=1000, \n",
    "        lr=0.1\n",
    ") -> tuple[Tensor, Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Performs linear regression using Stochastic Gradient Descent (SGD) with manual gradient calculation.\n",
    "\n",
    "    Parameters:\n",
    "        X: Input features of shape (S, F)\n",
    "        y: Target values of shape (S, 1)\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "\n",
    "    Returns:\n",
    "        tuple: (loss, weights, bias)\n",
    "    \"\"\"\n",
    "\n",
    "    (S, F) = X.shape\n",
    "\n",
    "    assert X.shape == Size([S, F])\n",
    "    assert y.shape == Size([S, 1])\n",
    "\n",
    "    w = randn((F, 1))\n",
    "    b = randn(1)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        predicted = X @ w + b\n",
    "        assert predicted.shape == Size([S, 1])\n",
    "\n",
    "        error = predicted - y\n",
    "        assert error.shape == Size([S, 1])\n",
    "\n",
    "        dL_dW = (2/S) * X.T @ error\n",
    "        assert dL_dW.shape == Size([F, 1])\n",
    "\n",
    "        w = w - lr * dL_dW\n",
    "        assert w.shape == Size([F, 1])\n",
    "\n",
    "        dL_db = (2/S) * error.sum()\n",
    "        assert dL_db.shape == Size([])\n",
    "\n",
    "        b = b - lr * dL_db\n",
    "        assert b.shape == Size([1])\n",
    "\n",
    "        #\n",
    "        # In the autograd version, computing the loss is required because it serves \n",
    "        # as the root of the computational graph for backpropagation. \n",
    "        #\n",
    "        # In the manualâ€‘gradient version, the loss value is not needed for the weight update itself,\n",
    "        # it is computed only to monitor training progress.\n",
    "        #\n",
    "        \n",
    "        loss = 1/S * (error ** 2).sum()\n",
    "        assert loss.shape == Size([])\n",
    "\n",
    "    return (loss, w, b)\n",
    "\n",
    "\n",
    "def _test_linear_regression_sgd_gradient(S: int, W: Tensor, B: Tensor, epochs=1000, lr=0.1) -> None:\n",
    "    \"\"\"\n",
    "    Tests the linear regression using Stochastic Gradient Descent (SGD) with manual gradient calculation, \n",
    "    by generating synthetic data with known weights, and verifies that the computed weights and bias are correct.\n",
    "\n",
    "    Parameters:\n",
    "        S: Number of samples\n",
    "        W: Model's weight(s)\n",
    "        B: Model's bias\n",
    "    \"\"\"\n",
    "\n",
    "    F = W.shape[0]\n",
    "    x = randn(S, F)\n",
    "    assert x.shape == Size([S, F])\n",
    "\n",
    "    y = x @ W + B\n",
    "    assert y.shape == Size([S, 1])\n",
    "\n",
    "    loss, w, b = linear_regression_sgd_gradient(x, y, epochs, lr)\n",
    "    assert equal(loss, 0.0)\n",
    "    assert equal(b, B)\n",
    "    assert equal(w, W)\n",
    "\n",
    "\n",
    "def test_linear_regression_sgd_gradient() -> None:\n",
    "    _test_linear_regression_sgd_gradient(10, W=T([[0.1]]), B=0.2)\n",
    "    _test_linear_regression_sgd_gradient(10, W=T([[0.3], [0.4]]), B=0.5)\n",
    "    _test_linear_regression_sgd_gradient(10, W=T([[0.6], [0.7], [0.8]]), B=0.9)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_linear_regression_sgd_gradient()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
