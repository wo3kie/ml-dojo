{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc3381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Project:\n",
    "#      PyTorch Dojo (https://github.com/wo3kie/ml-dojo)\n",
    "#\n",
    "# Author:\n",
    "#      Lukasz Czerwinski (https://www.lukaszczerwinski.pl/)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a164df",
   "metadata": {},
   "source": [
    "$$ \\text{Model} = M(W, b) = Wx+b $$\n",
    "$$ \\frac{\\partial M}{\\partial W} = x $$\n",
    "$$ \\frac{\\partial M}{\\partial b} = 1 $$\n",
    "$$ \\\\[2em] $$\n",
    "$$ \\text{Sigmoid} = S(M) = \\frac{e^M}{e^M+1} $$\n",
    "$$ \\frac{\\partial S}{\\partial M} = \\frac{e^M}{(e^M+1)^2} = S(1-S) $$\n",
    "$$ \\\\[2em] $$\n",
    "$$ \\text{Loss} = L(S) = -(y\\ln(S)+(1-y)\\ln(1-S)) $$\n",
    "$$ \\frac{\\partial L}{\\partial S} = -\\Big(y \\frac{1}{S} + (1-y) \\frac{1}{1-S}(-1) \\Big) = \\frac{S-y}{S(1-S)} $$\n",
    "$$ \\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial S} \\frac{\\partial S}{\\partial M} \\frac{\\partial M}{\\partial W} = \\frac{S-y}{S(1-S)} \\, S(1-S) \\, x = (S-y)x $$\n",
    "$$ \\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial S} \\frac{\\partial S}{\\partial M} \\frac{\\partial M}{\\partial b} = \\frac{S-y}{S(1-S)} \\, S(1-S) \\, 1 = S-y $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c764986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import exp, rand\n",
    "from torch.optim import SGD\n",
    "\n",
    "import import_ipynb\n",
    "from common import Patient, T # type: ignore\n",
    "\n",
    "\n",
    "def _Model(X, w, b):\n",
    "    return (X @ w.T) + b\n",
    "\n",
    "\n",
    "def _Sigmoid(m):\n",
    "    return exp(m) / (exp(m) + 1)\n",
    "\n",
    "\n",
    "def _Loss(S, y):\n",
    "    \"\"\" \n",
    "    BCE Loss - Binary Cross-Entropy Loss reduced by mean.\n",
    "    \"\"\"\n",
    "    \n",
    "    return -((y * S.log()) + ((1 - y) * (1 - S).log())).mean()\n",
    "\n",
    "\n",
    "def logistic_regression_sgd_gradient(X, y, epochs=2000, lr=0.01):\n",
    "    \"\"\"\n",
    "    Perform logistic regression using stochastic gradient descent with manual gradient computation.\n",
    "\n",
    "    Parameters:\n",
    "        X: Input features (shape: [S, F])\n",
    "        y: Target labels (shape: [S, 1])\n",
    "        epochs: Number of training iterations\n",
    "        lr: Learning rate\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the final loss and a prediction function that takes new input data and returns predicted probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    (S, F) = X.shape\n",
    "\n",
    "    w = rand(1, F)\n",
    "    b = rand(1)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        M = _Model(X, w, b)\n",
    "        S = _Sigmoid(M)\n",
    "        L = _Loss(S, y)\n",
    "\n",
    "        dL_dw = (S - y).T @ X\n",
    "        w = w - lr * dL_dw\n",
    "\n",
    "        dL_db = (S - y).sum()\n",
    "        b = b - lr * dL_db\n",
    "\n",
    "    return (L, lambda x: _Sigmoid(_Model(x, w, b)))\n",
    "\n",
    "\n",
    "def _test_logistic_regression_sgd_gradient(SL, ST, sick_threshold=0.5, epochs=2000, lr=0.01):\n",
    "    \"\"\"\n",
    "        Perform a test of the logistic regression implementation using synthetic patient data. \n",
    "        The function generates a dataset of patients, splits it into training and testing sets, \n",
    "        trains the logistic regression model on the training data, and evaluates its performance on the test data. \n",
    "        The test asserts that the false positive and false negative rates are below 5%.\n",
    "    \"\"\"\n",
    "\n",
    "    data = T([Patient(sick_threshold).data for _ in range(int(SL + ST))])\n",
    "    training_data = data[0: SL]\n",
    "    test_data = data[SL: SL + ST]\n",
    "\n",
    "    #\n",
    "    # Training\n",
    "    #\n",
    "\n",
    "    X = training_data[:, :-1].clone()\n",
    "    assert(X.shape == (SL, 5))\n",
    "\n",
    "    y = training_data[:, -1].clone().unsqueeze(1)\n",
    "    assert(y.shape == (SL, 1))\n",
    "\n",
    "    # Super simple data scaling to make the training numerically more stable.\n",
    "    X[:, 0] /= 100\n",
    "\n",
    "    (_, model) = logistic_regression_sgd_gradient(X, y, epochs, lr)\n",
    "\n",
    "    #\n",
    "    # Testing\n",
    "    #\n",
    "\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for d in test_data:\n",
    "        X = d[:-1].clone()\n",
    "        assert(X.shape == (5,))\n",
    "\n",
    "        y = d[-1].clone()\n",
    "        assert(y.shape == ())\n",
    "\n",
    "        # The same data scaling as during training.\n",
    "        X[0] /= 100\n",
    "\n",
    "        predicted = model(X)\n",
    "\n",
    "        is_sick_predicted = predicted >= T(0.5)\n",
    "        is_sick_actual = y == T(1)\n",
    "\n",
    "        if is_sick_predicted and not is_sick_actual:\n",
    "            false_positives += 1\n",
    "        elif not is_sick_predicted and is_sick_actual:\n",
    "            false_negatives += 1\n",
    "\n",
    "    assert(false_positives / ST <= 0.05)\n",
    "    assert(false_negatives / ST <= 0.05)\n",
    "\n",
    "\n",
    "def test_logistic_regression_sgd_gradient():\n",
    "    _test_logistic_regression_sgd_gradient(80, 20, 0.1)\n",
    "    _test_logistic_regression_sgd_gradient(80, 20, 0.3)\n",
    "    _test_logistic_regression_sgd_gradient(80, 20, 0.5)\n",
    "    _test_logistic_regression_sgd_gradient(80, 20, 0.7)\n",
    "    _test_logistic_regression_sgd_gradient(80, 20, 0.9)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_logistic_regression_sgd_gradient()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
