{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc3381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Project:\n",
    "#      PyTorch Dojo (https://github.com/wo3kie/ml-dojo)\n",
    "#\n",
    "# Author:\n",
    "#      Lukasz Czerwinski (https://www.lukaszczerwinski.pl/)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a164df",
   "metadata": {},
   "source": [
    "$$ \\text{Model} = M(W, b) = Wx+b $$\n",
    "$$ \\frac{\\partial M}{\\partial W} = x $$\n",
    "$$ \\frac{\\partial M}{\\partial b} = 1 $$\n",
    "$$ \\\\[2em] $$\n",
    "$$ \\text{Sigmoid} = S(M) = \\frac{e^M}{e^M+1} $$\n",
    "$$ \\frac{\\partial S}{\\partial M} = \\frac{e^M}{(e^M+1)^2} = S(1-S) $$\n",
    "$$ \\\\[2em] $$\n",
    "$$ \\text{Loss} = L(S) = -(y\\ln(S)+(1-y)\\ln(1-S)) $$\n",
    "$$ \\frac{\\partial L}{\\partial S} = -\\Big(y \\frac{1}{S} + (1-y) \\frac{1}{1-S}(-1) \\Big) = \\frac{S-y}{S(1-S)} $$\n",
    "$$ \\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial S} \\frac{\\partial S}{\\partial M} \\frac{\\partial M}{\\partial W} = \\frac{S-y}{S(1-S)} \\, S(1-S) \\, x = (S-y)x $$\n",
    "$$ \\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial S} \\frac{\\partial S}{\\partial M} \\frac{\\partial M}{\\partial b} = \\frac{S-y}{S(1-S)} \\, S(1-S) \\, 1 = S-y $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c764986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import exp, rand, Tensor\n",
    "\n",
    "import import_ipynb\n",
    "from common import Patient, T # type: ignore\n",
    "\n",
    "\n",
    "def _Linear(X: Tensor, w: Tensor, b: Tensor) -> Tensor:\n",
    "    return X @ w.T + b\n",
    "\n",
    "\n",
    "def _Model(X: Tensor, w: Tensor, b: Tensor) -> Tensor:\n",
    "    return _Linear(X, w, b)\n",
    "\n",
    "\n",
    "def _Sigmoid(m: Tensor) -> Tensor:\n",
    "    return exp(m) / (exp(m) + 1)\n",
    "\n",
    "\n",
    "def _BinaryCrossEntropy(S: Tensor, y: Tensor) -> Tensor:\n",
    "    return -((y * S.log()) + ((1 - y) * (1 - S).log())).mean()\n",
    "\n",
    "\n",
    "def _Loss(S: Tensor, y: Tensor) -> Tensor:  \n",
    "    return _BinaryCrossEntropy(S, y)\n",
    "\n",
    "\n",
    "def logistic_regression_sgd_gradient(\n",
    "        X: Tensor, \n",
    "        y: Tensor, \n",
    "        epochs=2000, \n",
    "        lr=0.1\n",
    ") -> tuple[float, callable]:\n",
    "    \"\"\"\n",
    "    Perform logistic regression using Stochastic Gradient Descent (SGD) with manual gradient calculation..\n",
    "\n",
    "    Parameters:\n",
    "        X: Input features of shape (S, F)\n",
    "        y: Target values of shape (S, 1)\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the final loss and a prediction function that takes new input data and returns predicted probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    (s, f) = X.shape\n",
    "\n",
    "    w = rand(1, f)\n",
    "    assert w.shape == (1, f)\n",
    "\n",
    "    b = rand(1)\n",
    "    assert b.shape == (1,)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        M = _Model(X, w, b)\n",
    "        assert M.shape == (s, 1)\n",
    "\n",
    "        S = _Sigmoid(M)\n",
    "        assert S.shape == (s, 1)\n",
    "\n",
    "        dL_dw = (S - y).T @ X\n",
    "        assert dL_dw.shape == (1, f)\n",
    "\n",
    "        dL_db = (S - y).mean()\n",
    "        assert dL_db.shape == ()\n",
    "\n",
    "        w = w - lr * dL_dw\n",
    "        b = b - lr * dL_db\n",
    "        \n",
    "        #\n",
    "        # In the autograd version, computing the loss is required because it serves \n",
    "        # as the root of the computational graph for backpropagation. \n",
    "        #\n",
    "        # In the manualâ€‘gradient version, the loss value is not needed for the weight update itself,\n",
    "        # it is computed only to monitor training progress.\n",
    "        #\n",
    "\n",
    "        L = _Loss(S, y)\n",
    "\n",
    "    return (L.item(), lambda x: _Sigmoid(_Model(x, w, b)))\n",
    "\n",
    "\n",
    "def _test_logistic_regression_sgd_gradient(SL, ST, sick_threshold=0.5, epochs=2000, lr=0.1) -> None:\n",
    "    \"\"\"\n",
    "        Perform a test of the logistic regression implementation using synthetic patient data. \n",
    "\n",
    "        The function generates a dataset of patients, splits it into training and testing sets, \n",
    "        trains the logistic regression model on the training data, and evaluates its performance on the test data. \n",
    "        The test asserts that achieved accuracy is at least 90%.\n",
    "\n",
    "        Parameters:\n",
    "            SL: Number of samples in the training dataset\n",
    "            ST: Number of samples in the test dataset\n",
    "            sick_threshold: Threshold for determining if a patient is sick based on their data\n",
    "            epochs: Number of training epochs for the logistic regression model\n",
    "            lr: Learning rate for the logistic regression model\n",
    "    \"\"\"\n",
    "\n",
    "    #\n",
    "    # Dataset generation\n",
    "    #\n",
    "\n",
    "    training_data = T([Patient(sick_threshold).data for _ in range(SL)])\n",
    "    test_data = T([Patient(sick_threshold).data for _ in range(ST)])\n",
    "    (_, f) = (training_data.shape[0], training_data.shape[1] - 1)\n",
    "\n",
    "    #\n",
    "    # Training\n",
    "    #\n",
    "\n",
    "    X = training_data[:, :-1]\n",
    "    X[:, 0] /= 100 # Data scaling to make training numerically stable\n",
    "    assert(X.shape == (SL, f))\n",
    "\n",
    "    y = training_data[:, -1].unsqueeze(1)\n",
    "    assert(y.shape == (SL, 1))\n",
    "\n",
    "    (_, model) = logistic_regression_sgd_gradient(X, y, epochs, lr)\n",
    "\n",
    "    #\n",
    "    # Testing\n",
    "    #\n",
    "\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "\n",
    "    for d in test_data:\n",
    "        X = d[:-1]\n",
    "        X[0] /= 100 # The same data scaling as during training.\n",
    "        assert(X.shape == (f,))\n",
    "\n",
    "        y = d[-1]\n",
    "        assert(y.shape == ())\n",
    "\n",
    "        predicted = model(X)\n",
    "\n",
    "        is_sick_predicted = (predicted >= T(0.5))\n",
    "        is_sick_actual = (y == T(1))\n",
    "\n",
    "        if is_sick_predicted and is_sick_actual:\n",
    "            true_positives += 1\n",
    "        elif (not is_sick_predicted) and (not is_sick_actual):\n",
    "            true_negatives += 1\n",
    "        \n",
    "    accuracy = (true_positives + true_negatives) / ST\n",
    "    assert accuracy >= 0.9\n",
    "\n",
    "\n",
    "def test_logistic_regression_sgd_gradient():\n",
    "    _test_logistic_regression_sgd_gradient(80, 20, 0.1)\n",
    "    _test_logistic_regression_sgd_gradient(80, 20, 0.3)\n",
    "    _test_logistic_regression_sgd_gradient(80, 20, 0.5)\n",
    "    _test_logistic_regression_sgd_gradient(80, 20, 0.7)\n",
    "    _test_logistic_regression_sgd_gradient(80, 20, 0.9)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_logistic_regression_sgd_gradient()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
