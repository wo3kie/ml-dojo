{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc3381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Project:\n",
    "#      PyTorch Dojo (https://github.com/wo3kie/ml-dojo)\n",
    "#\n",
    "# Author:\n",
    "#      Lukasz Czerwinski (https://www.lukaszczerwinski.pl/)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a164df",
   "metadata": {},
   "source": [
    "$$ \\text{Model} = M(W, b) = Wx+b $$\n",
    "$$ \\frac{\\partial M}{\\partial W} = x $$\n",
    "$$ \\frac{\\partial M}{\\partial b} = 1 $$\n",
    "$$ \\\\[2em] $$\n",
    "$$ \\text{Sigmoid} = S(M) = \\frac{e^M}{e^M+1} $$\n",
    "$$ \\frac{\\partial S}{\\partial M} = \\frac{e^M}{(e^M+1)^2} = S(1-S) $$\n",
    "$$ \\\\[2em] $$\n",
    "$$ \\text{Loss} = L(S) = -(y\\ln(S)+(1-y)\\ln(1-S)) $$\n",
    "$$ \\frac{\\partial L}{\\partial S} = -\\Big(y \\frac{1}{S} + (1-y) \\frac{1}{1-S}(-1) \\Big) = \\frac{S-y}{S(1-S)} $$\n",
    "$$ \\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial S} \\frac{\\partial S}{\\partial M} \\frac{\\partial M}{\\partial W} = \\frac{S-y}{S(1-S)} \\, S(1-S) \\, x = (S-y)x $$\n",
    "$$ \\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial S} \\frac{\\partial S}{\\partial M} \\frac{\\partial M}{\\partial b} = \\frac{S-y}{S(1-S)} \\, S(1-S) \\, 1 = S-y $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c764986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import exp, rand, Tensor\n",
    "\n",
    "import import_ipynb\n",
    "from common import assert_eq, assert_ge, assert_lt, Patient, T # type: ignore\n",
    "\n",
    "def _Linear(X: Tensor, w: Tensor, b: Tensor) -> Tensor:\n",
    "    return X @ w.T + b\n",
    "\n",
    "\n",
    "def _Model(X: Tensor, w: Tensor, b: Tensor) -> Tensor:\n",
    "    return _Linear(X, w, b)\n",
    "\n",
    "\n",
    "def _Sigmoid(m: Tensor) -> Tensor:\n",
    "    return exp(m) / (exp(m) + 1)\n",
    "\n",
    "\n",
    "def _BinaryCrossEntropy(S: Tensor, y: Tensor) -> Tensor:\n",
    "    return -((y * S.log()) + ((1 - y) * (1 - S).log())).mean()\n",
    "\n",
    "\n",
    "def _Loss(S: Tensor, y: Tensor) -> Tensor:  \n",
    "    return _BinaryCrossEntropy(S, y)\n",
    "\n",
    "\n",
    "def logistic_regression_sgd_gradient(X: Tensor, y: Tensor, epochs=2000, lr=0.1) -> tuple[float, callable]:\n",
    "    \"\"\"\n",
    "    Perform logistic regression using Stochastic Gradient Descent (SGD) with manual gradient calculation..\n",
    "\n",
    "    Parameters:\n",
    "        X: Input features of shape (Samples, Features)\n",
    "        y: Target values of shape (Samples, 1)\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the final loss and a prediction function that takes new input data and returns predicted probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    (s, f) = X.shape\n",
    "\n",
    "    w = rand(1, f)\n",
    "    assert_eq(w.shape, (1, f))\n",
    "\n",
    "    b = rand(1)\n",
    "    assert_eq(b.shape, (1,))\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        M = _Model(X, w, b)\n",
    "        assert_eq(M.shape, (s, 1))\n",
    "\n",
    "        S = _Sigmoid(M)\n",
    "        assert_eq(S.shape, (s, 1))\n",
    "\n",
    "        dL_dw = (S - y).T @ X\n",
    "        assert_eq(dL_dw.shape, (1, f))\n",
    "\n",
    "        #\n",
    "        # When reducing the loss function, using `mean()` is more appropriate than `sum()` because \n",
    "        # it normalizes the loss by the number of samples, providing a more stable and comparable \n",
    "        # loss value across different batch sizes.\n",
    "        #\n",
    "        dL_db = (S - y).mean()\n",
    "        assert_eq(dL_db.shape, ())\n",
    "\n",
    "        w = w - lr * dL_dw\n",
    "        b = b - lr * dL_db\n",
    "        \n",
    "        #\n",
    "        # In the autograd version, computing the loss is required because it serves \n",
    "        # as the root of the computational graph for backpropagation. \n",
    "        #\n",
    "        # In the manualâ€‘gradient version, the loss value is not needed for the weight update itself,\n",
    "        # it is computed only to monitor training progress.\n",
    "        #\n",
    "\n",
    "        L = _Loss(S, y)\n",
    "\n",
    "    return (L.item(), lambda x: _Sigmoid(_Model(x, w, b)))\n",
    "\n",
    "\n",
    "def _test_logistic_regression_sgd_gradient(epochs=2000, lr=0.1) -> None:\n",
    "    training_data = T([Patient(0.5).data for _ in range(80)])\n",
    "\n",
    "    X = training_data[:, :-1]\n",
    "    X[:, 0] /= 100 # Data scaling to make training numerically stable\n",
    "    y = training_data[:, -1].unsqueeze(1)\n",
    "\n",
    "    (_, model) = logistic_regression_sgd_gradient(X, y, epochs, lr)\n",
    "\n",
    "    for d in T([Patient(1.0).data for _ in range(10)]):\n",
    "        d[0] /= 100 # The same data scaling as during training.\n",
    "        assert_ge(model(d[:-1]), T(0.5))\n",
    "        \n",
    "    for d in T([Patient(0.0).data for _ in range(10)]):\n",
    "        d[0] /= 100 # The same data scaling as during training.\n",
    "        assert_lt(model(d[:-1]), T(0.5))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _test_logistic_regression_sgd_gradient()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
